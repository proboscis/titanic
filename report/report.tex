\input{./Templates/header}

\title{データサイエンス第一回課題}
\author{増井 建斗 48156621}
\date{\today}
\begin{document}
\maketitle

%======================================================================
\section{概要}
多層パーセプトロンを用いてオンラインニュースの人気予測を行った．多層パーセプトロンの他，線形回帰等のモデルの構造とデータの前処理がニュースの人気予測精度に及ぼす影響について考察する．

%======================================================================
\section{提出した予測モデルの構成}
提出した予測モデルは，データセットに対して前処理を加えずに学習した多層パーセプトロンである．モデルの構造は表\ref{table:model}に示す．モデルの学習にはAdam\cite{adam}を用いた．
\inputtable{model}{多層パーセプトロンモデルの構造}
Activation Functionには，Exponential Linear Units(ELU)\cite{elu}を用いた．ELUは以下の式で定義される．
\begin{equation}
f(x) = \begin{cases}
	x & \text{if }x \ge 0\\
	\alpha(\exp{x}-1) & \text{if }x  < 0
	\end{cases}
\end{equation}
\begin{equation}
f'(x) = \begin{cases}
	1 & \text{if }x \ge 0\\
	f(x)+\alpha & \text{if }x  < 0
	\end{cases}
\end{equation}
%======================================================================
\section{予測精度を向上させるための工夫点と結果，考察}
\inputtable{effect}{前処理とモデルによる予測精度の関係}

予測精度を向上させるため，前処理として特徴選択，Log scaling,主成分分析を行い，それぞれが予測精度に与える影響を確認した．線形回帰モデル(LR)と多層パーセプトロン(MLP)の２つのモデルを用い，予測モデルと前処理それぞれの予測精度に対する影響も確認した．３種類の前処理それぞれと予測精度の関係を表\ref{table:effect}に示す．値はそれぞれvalidationセットに対するMean Squared Error(MSE), Mean Absolute Error(MAE)を表し，4 fold cross validationの平均と標準偏差である．ただし，MLPについては30 fold cross validationの結果を載せている．以下，それぞれの効果について考察する．

\subsection{特徴選択}
特徴選択では，入力データから曜日の情報を除去する操作を行った．図\ref{fig:raw_feature}に，データセットの特徴量ごとの分布を示す．曜日を示す\texttt{weekday\_is\_monday}から\texttt{weekday\_is\_sunday}までの特徴は，特徴と目的変数の相関が低かったため消去することで予測精度を向上させることが出来るのではないかと考えた．表\ref{table:effect}における曜日情報なしの項を確認すると，LR(MSE)については若干の精度向上が確認できるが，LR(MAE)においてはむしろ精度が悪化している．従って，期待した精度向上の効果は得られなかった．
\subsection{Log scaling}
図\ref{fig:raw_feature}を確認すると，\texttt{n\_tokens\_content}等，幾つかの特徴量のスケールが指数的であることが確認できる．線形モデルでは非線形な指数的スケールを持った特徴から回帰を行うことが難しいだろうと考え，それらの特徴のlogをとる処理を行った．この処理を行った後の特徴量の分布が，図\ref{fig:log_feature}である．この処理は期待に反して予測精度を下げる結果となった．
\subsection{主成分分析(PCA)}
主成分分析を用いて入力変数を変換することで，入力変数の線形和で目的変数を表現しやすくなるのではないかと考えた．しかし，この前処理も期待に反して予測精度を下げる結果となった．この理由は，PCAは入力変数を表現しやすい射影を計算するのみで，目的変数の回帰に有益な射影を得ることが出来るとは限らないためだと考察する．

% Fibgure \ref{graph:samplegraph} is an example of a sample graph."

\inputfig{raw_feature}{学習データ・セットの各特徴ヒストグラム}{0.4}
\inputfig{log_feature}{学習データセットの一部について，logををとったもの}{0.4}

% \inputgraph{samplegraph}{Caption for the sample graph.}{0.8}
%======================================================================
\section{データ分析結果}
\subsection{目的変数分布の一様化}
回帰対象となっている，sharesの自然対数を取った結果のヒストグラムを図\ref{fig:log_y_hist}に示す．図\ref{fig:log_y_hist}から，目的変数の生起確率は一様となっていないことがわかる．従って，学習モデルは生起確率が高い値を出力するように収束すると考えられる．目的変数のスケールが指数的であるため正規確率の低い部分に80万などの非常に大きな値が含まれているが，この値の予測に失敗すると，テストデータセットの標本サイズによっては非常に大きなMAEが現れる．今回の場合標本サイズは10000であるため，80万に対して1万と１回予測するだけでMAEに79の影響を与えてしまう．これを回避するためには，正規確率が低い値を正確に予測する必要があり，目的変数の分布を一様にすることで正規確率が低い値の予測精度を上げることが出来ると考えた．実際に目的変数の分布が一様になるようにデータ・セットをサンプルしなおし，回帰学習を行った結果正規確率の低い値の予測精度を上げることが出来た．しかし実際に最適化したい，元の分布の目的変数に対しては，より高いMAEを出力することになった．これはもちろん，実際の目的変数の分布と最適化に用いた目的変数の分布が異なるためである．従って，単純に分布を一様にするだけでは正規確率の低い値に対する予測精度とMAEの最小化を両立することが出来ないとわかる．
\inputfig{log_y_hist}{目的変数の分布}{0.4}
\subsection{正則化パラメータの関係}
LRによる回帰では，正則化パラメータを用いなかった．MLPについては，l1,l2,l1l2,dropoutの効果を確認した．MLPでは正則化を用いない場合，ある程度学習が進んだ後に学習データに対するMAEを減少させつつValidationデータに対するMAEを増大し続ける，過学習をする傾向が確認できた．これに対してそれぞれの正則化項を追加し再度学習を行った場合，全ての正則化について学習データとValidationデータが逆の方向に増減する傾向が抑制され，過学習が抑制されることが確認できた．それぞれの正則化については収束の速度に差異が見られることが確認でき，特にl1正則化のみを用いた場合には学習が比較的遅くなることが確認出来た．
\subsection{Stacked Autoencoder}
Stacked Autoencoderによるpretrainingによって予測精度が向上するか検討したが，Autoencoderを用いない場合と差異が見られなかった．
\subsection{Order Classification}
目的変数が指数的な値をとっていることに着目し，まず目的変数の指数を識別し，その値を追加の特徴量として目的変数の回帰を行うことで予測精度が向上出来るのでは無いかと考えた．目的変数の指数識別にはMLPを用いたが，予測された指数から実際の値に戻すのみでもMAEは3000台が得られた．この識別された値を追加の特徴量として新たなMLPを用いて回帰を行ったが，結果は指数を用いない場合と全く同じとなった．
%======================================================================
\section{ここまでの講義の感想，要望}
現在自分が研究している分野の授業だが，様々な見逃していた情報を学習することが出来た．また，実際にpythonでのコーディングを行うときに気をつけるべき点が示されており，大変参考になっている．

%======================================================================
\bibliography{references.bib}
\end{document}
